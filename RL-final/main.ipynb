{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIPfpOT8IM_o",
        "outputId": "2f761b5e-955d-4992-a927-c141fd5cfb74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ub3XGKuXJy4d",
        "outputId": "4635a684-dbc2-4471-ead9-9d0ec31cbd2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[main 6689aa8] abc\n",
            " 18 files changed, 282 insertions(+), 10 deletions(-)\n",
            " create mode 100644 __pycache__/final_torch_model.cpython-310.pyc\n",
            " create mode 100644 __pycache__/myNet.cpython-310.pyc\n",
            " create mode 100644 __pycache__/optimize.cpython-310.pyc\n",
            " create mode 100644 __pycache__/policy.cpython-310.pyc\n",
            " create mode 100644 __pycache__/replayBuffer.cpython-310.pyc\n",
            " create mode 100644 __pycache__/torch_model.cpython-310.pyc\n",
            " create mode 100644 __pycache__/train.cpython-310.pyc\n",
            " create mode 100644 models/blue_8.pt\n",
            " copy models/{blue_dqn_final.pt => blue_dqn_fina2l.pt} (100%)\n",
            " rewrite models/blue_dqn_final.pt (95%)\n",
            " create mode 100644 optimize.py\n",
            " create mode 100644 policy.py\n",
            " create mode 100644 replayBuffer.py\n",
            " create mode 100644 video/new_pretrained.mp4\n",
            " create mode 100644 video/pretrained.mp4\n",
            " create mode 100644 video/random.mp4\n",
            "Enumerating objects: 26, done.\n",
            "Counting objects: 100% (26/26), done.\n",
            "Delta compression using up to 2 threads\n",
            "Compressing objects: 100% (21/21), done.\n",
            "Writing objects: 100% (21/21), 1.71 MiB | 3.27 MiB/s, done.\n",
            "Total 21 (delta 3), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (3/3), completed with 1 local object.\u001b[K\n",
            "To https://github.com/phamtiep/RL-final.git\n",
            "   7899b2c..6689aa8  main -> main\n"
          ]
        }
      ],
      "source": [
        "!git add .\n",
        "!git commit -m \"abc\"\n",
        "!git push origin main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yIoGxZbKVzFL"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!git clone https://github.com/phamtiep/RL-final\n",
        "\n",
        "%cd RL-final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvOZJPlz9sCx",
        "outputId": "b9402f60-add7-403b-fb1f-cd06c31412f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "Everything up-to-date\n"
          ]
        }
      ],
      "source": [
        "!git add .\n",
        "!git commit -m 'update'\n",
        "!git push"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QMz8losUkQ2",
        "outputId": "7d69aa45-0df3-445f-93bb-bd216bfb9b2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/Farama-Foundation/MAgent2\n",
            "  Cloning https://github.com/Farama-Foundation/MAgent2 to /tmp/pip-req-build-sxgpkk7x\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Farama-Foundation/MAgent2 /tmp/pip-req-build-sxgpkk7x\n",
            "  Resolved https://github.com/Farama-Foundation/MAgent2 to commit b2ddd49445368cf85d4d4e1edcddae2e28aa1406\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<2.0,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from magent2==0.3.3) (1.26.4)\n",
            "Requirement already satisfied: pygame>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from magent2==0.3.3) (2.6.1)\n",
            "Requirement already satisfied: pettingzoo>=1.23.1 in /usr/local/lib/python3.10/dist-packages (from magent2==0.3.3) (1.24.3)\n",
            "Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo>=1.23.1->magent2==0.3.3) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/Farama-Foundation/MAgent2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ladedZlkUrtd"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict, deque\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from magent2.environments import battle_v4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "onpB6o4zUv-q"
      },
      "outputs": [],
      "source": [
        "from replayBuffer import MultiAgentReplayBuffer\n",
        "from optimize import *\n",
        "from myNet import QNetwork\n",
        "from policy import epsilon_greedy_policy\n",
        "from train import train_agents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gtwkN1_UjXh",
        "outputId": "bd11d715-9660-40c2-993c-fe801046a287"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "No model found!\n"
          ]
        }
      ],
      "source": [
        "env = battle_v4.env(map_size=45, minimap_mode=False, step_reward=0.01,\n",
        "                        dead_penalty=-2, attack_penalty=-0.1, attack_opponent_reward=2,\n",
        "                        max_cycles=300, extra_features=False, render_mode=\"rgb_array\")\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.9\n",
        "EPS_START = 1\n",
        "EPS_END = 0.1\n",
        "EPS_DECAY = 50\n",
        "TAU = 0.005\n",
        "LR = 1e-4\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "observation_shape = env.observation_space(\"blue_0\").shape\n",
        "action_shape = env.action_space(\"blue_0\").n\n",
        "\n",
        "policy_net = QNetwork(observation_shape, action_shape).to(device)\n",
        "red_policy_net = QNetwork(observation_shape, action_shape).to(device)\n",
        "target_net = QNetwork(observation_shape, action_shape).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
        "\n",
        "pretrained_net = QNetwork(observation_shape, action_shape).to(device)\n",
        "pretrained_net.load_state_dict(torch.load(\"models/red.pt\", map_location=device, weights_only=True))\n",
        "\n",
        "try:\n",
        "    checkpoint = torch.load(\"models/blue_2.pt\", map_location=device, weights_only=True)\n",
        "    policy_net.load_state_dict(checkpoint[\"policy_net_state_dict\"])\n",
        "    target_net.load_state_dict(checkpoint[\"target_net_state_dict\"])\n",
        "    red_policy_net.load_state_dict(checkpoint[\"policy_net_state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "    episode = checkpoint[\"episode\"]\n",
        "    print(f\"Start with episode: {episode}\")\n",
        "except Exception as e:\n",
        "    print(f\"No model found!\")\n",
        "    episode = 0\n",
        "\n",
        "buffer = MultiAgentReplayBuffer(10000, observation_shape, action_shape)\n",
        "steps_done = episode\n",
        "episode_rewards = []\n",
        "episode_losses = []\n",
        "running_loss = 0.0\n",
        "num_episodes = 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "lmX8UtGCVSwp",
        "outputId": "10c55087-3814-40a1-b6f9-bd19f5a39b1b"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-bb4b648ec194>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_agents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mred_policy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_net\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTAU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPS_DECAY\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mEPS_DECAY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPS_END\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEPS_END\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPS_START\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEPS_START\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/RL-final/train.py\u001b[0m in \u001b[0;36mtrain_agents\u001b[0;34m(env, num_episodes, policy_net, target_net, red_policy_net, buffer, optimizer, steps_done, episode, pretrained_net, TAU, BATCH_SIZE, GAMMA, EPS_START, EPS_END, EPS_DECAY, device)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                     \u001b[0;31m# Perform one step of the optimization (on the policy network)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                     \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0;31m# Soft update of the target network's weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/RL-final/optimize.py\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m(policy_net, target_net, optimizer, buffer, BATCH_SIZE, GAMMA, device)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Calculate target Q-values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mnext_state_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mnon_final_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdone_batch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Create a mask for non-terminal states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Only compute for non-terminal states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_agents(env, num_episodes, policy_net, target_net, red_policy_net, buffer, optimizer, steps_done, episode, pretrained_net,TAU, EPS_DECAY= EPS_DECAY, EPS_END = EPS_END, EPS_START = EPS_START, device = device, BATCH_SIZE = BATCH_SIZE, GAMMA = GAMMA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "woPoPkWtmNSZ"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load(\"models/blue_8.pt\", map_location=device, weights_only=True)\n",
        "a = checkpoint[\"policy_net_state_dict\"]\n",
        "torch.save(a, 'models/blue_dqn_final.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJNzzrwxfqsm",
        "outputId": "95f5196c-a3e3-4855-8f86-77cc736696fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done recording random agents\n",
            "Done recording pretrained agents\n",
            "Done recording new pretrained agents\n"
          ]
        }
      ],
      "source": [
        "!python demo.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLoNrPhzWl5S"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
